## 问题概述

无约束优化形式如下：

$$
\min f(x) \\
s.t. x \in \mathbb{R}^n
$$

1. 在实际问题中，我们往往会遇到无约束优化问题，如

$$
\min \| Ax - b \|^2
$$

2. 对于约束优化问题可以采用方法将其转化为无约束优化问题，如拉格朗日乘子法。

3. 最优解的定义
   1. 局部最优解：$\forall x \in \text{dom}f \cap \text{N}_\delta (x), f(x^\star) \leq f(x)$
   2. 全局最优解：$\forall x \in \text{dom}f, f(x^\star) \leq f(x)$
   3. 严格局部最优解：$\forall x \in \text{dom}f \cap \text{N}_\delta (x), f(x^\star) < f(x)$

## 收敛速度

如果对于凸二次优化，能在有限步数内收敛到最优解，那么我们称其为**二次收敛**。

### Quotient Convergence Rate

即我们考察

$$
\lim_{k \to \infin} \frac{\| x_{k+1} - x^\star \|}{\| x_k - x^\star \|^p} = \lambda
$$

1. $p = 1,\lambda = 1$，称为**次线性收敛**
2. $p = 1,\lambda \in (0,1)$，称为**线性收敛**
3. $p = 1,\lambda = 0$，称为**超线性收敛**
4. $p = 2,\lambda \in (0, +\infin)$，称为**二次收敛**

### Root Convergence Rate

是一种曲线救国的收敛速度，即

$$
\| x_k - x^\star \| \leq t_k
$$

$t_k$ 也是一个序列，使用放缩的方法，来保证收敛性。

### 迭代次数

如果我们指定一个精度$\varepsilon$，那么对于不同的收敛速度，我们需要的迭代次数是不同的。

例如，对于次线性收敛，我们需要迭代次数为$\frac{1}{\varepsilon}$，对于线性收敛，我们需要迭代次数为$\log \frac{1}{\varepsilon}$，对于二次收敛，我们需要迭代次数为$\sqrt{\frac{1}{\varepsilon}}$。

## 算法

迭代下降算法，给定初始点$x_0$，迭代产生点列$\{x^k\}_{k=1}^N$，其中$d_k$是搜索方向，$\alpha_k$是步长, **并且满足$f(x_{k+1}) \leq f(x_{k-m})$（单调性）**

那么如何迭代$x_{k+1}$

1.  **线搜索方法(Line Search Method)**

    先定一个搜索方向$d_k^\star$，然后在该方向上搜索最优步长$\alpha_k \geq 0$。

    $$
    x_{k+1} = x_k + \alpha_k d_k^\star
    $$

    步长同时也要确定，因为步长太大可能会导致不收敛，步长太小可能会导致收敛速度慢。

    名字得来是因为确定方向后，我们在该方向上搜索最优步长，是一个一维搜索问题。

    $$
    \min f(x_k + \alpha d_k^\star)
    $$

2.  **信赖域方法(Trust Region Method)**

    同时确定搜索方向和步长。是指先确定一个信赖域$\Delta$，然后在该信赖域内搜索最优解。

    $$
    \min f(x_k + d) \\
    s.t. \| d \| \leq \Delta
    $$

    信赖域方法的优点是可以保证收敛性，但是信赖域的大小也是一个问题。因此需要近似,

    $$
    f(x_k + d) \approx f(x_k) + \nabla f(x_k)^T d + \frac{1}{2} d^T H_k d \\
    \approx f(x_k) + \nabla f(x_k)^T d + \frac{1}{2} d^T B_k d \\
    s.t. \| d \| \leq \Delta
    $$

## 线搜索方法的基本思路

### 下降方向的判定

$$
x_{k+1} = x_k + \alpha_k d_k
$$

我们对迭代方法进行一些假设：

1. $\{ f(x_k)\}_{k=1}^\infin$是单调下降的
2. $\alpha_k$ 足够小，并且$\| d_k \| = 1$

则我们可以对$f(x_{k+1})$进行展开

$$
f(x_{k+1}) = f(x_k + \alpha_k d_k) = f(x_k) + \alpha_k \nabla f(x_k)^T d_k + o(\| \alpha_k d_k \|) \\
\iff f(x_{k+1}) - f(x_k) = \alpha_k \nabla f(x_k)^T d_k + o(\alpha_k) \\
\iff \nabla f(x_k)^T d_k < 0
$$

能够保证单调递减

但其实可以使用方向导数说明对下降方向进行判定的方法:

$$
\lim_{\varDelta x \to 0, \varDelta y \to 0} \frac{f(x + \varDelta x, y + \varDelta y) - f(x,y)}{\sqrt{\varDelta x^2 + \varDelta y^2}} \\
\iff \lim_{\varDelta t \to 0} \frac{f(x + \varDelta t \cos \theta, y + \varDelta t \sin \theta) - f(x, y)}{\varDelta t } \\
= \lim_{\varDelta t \to 0} \frac{\frac{\partial f}{\partial x}\varDelta t \cos \theta + \frac{\partial f}{\partial y}\varDelta t \sin \theta + o(\varDelta t)}{\varDelta t} = \nabla f(x,y) \cdot (\cos \theta, \sin \theta) \\ = \nabla f(x,y) \cdot d
$$

**因此我们得到了一个结论**，如果$\nabla f(x_k)^T d_k < 0$，那么$f(x_{k+1}) < f(x_k)$

而固定步长的情况下，使用$- \nabla f(x)$为方向**保证在单个点处最速下降**。

### 算法的框架

1. **初始化**：给定初始点$x_0$，迭代次数$N$，初始步长$\alpha_0$，初始搜索方向$d_0$
2. Judge $x_i$是否满足终止条件，满足则停止迭代
3. **搜索方向**：$d_k$
4. **线搜索**：$\alpha_k = \arg \min f(x_k + \alpha d_k)$
5. 回到第 2 步

## 线搜索方法——判定步长

假定我们已经确定了搜索方向$d_k$，那么如何确定步长$\alpha_k$呢？

### 1. 精确线搜索

$$
\alpha_k = \arg \min_\alpha f(x_k + \alpha d_k) = \\
\arg \min_\alpha \phi(\alpha)
$$

我们对函数的性质进行研究：

$$
\phi(0) = f(x_k) \\
\phi^{'}(\alpha) = \nabla f(x_k + \alpha d_k)^T d_k \\
\phi^{'}(0) = \nabla f(x_k)^T d_k < 0
$$

最优解$\alpha_k^\star$的求解复杂度太高。

但是其实不用寻找最优解，只需要找到一个满足条件的$\alpha_k$即可。

## 2. 非确定性线搜索

### 选择$\alpha$的必要条件

寻找一个满足条件的$\alpha_k$，使得

$$
\phi(\alpha_k) = f(x_k + \alpha_k d_k) \leq f(x_k)  = \phi(0)
$$

但是其实这个条件只是一个**收敛到最优解的必要条件**，是一个很松的条件，因此我们需要一个更强的条件。

### 选择$\alpha$的充分条件——Armijo Condition

如果我们利用$\phi(0)$的一阶信息，我们能够得到一个切线

$$
\phi(\alpha) \approx \phi(0) + \nabla f(x_k)^T d_k \alpha
$$

那么我们只要寻找一个$\alpha_k$，使得

$$
\phi(\alpha_k) \leq \phi(0) + \nabla f(x_k)^T d_k \alpha_k
$$

可以得到一个很强的条件，但是并不是所有的函数存在满足条件的$\alpha_k$。

因此我们需要进行一些修正，使得$\alpha_k$满足条件。

$$
l_k(\alpha) = \phi(0) + c_1 \alpha \nabla f(x_k)^T d_k , c_1 \in (0, 1) \\
$$

我们寻找一个$\alpha_k$，使得

$$
\phi(\alpha_k) \leq l_k(\alpha_k)
$$

这是一个松的条件和一个强的条件的结合, 叫做**Armijo Condition**。

![img](https://img2023.cnblogs.com/blog/3436855/202405/3436855-20240518010604933-1197101468.png)

然而 Armijo Condition 可能会得到很小的$\alpha_k$，因此我们需要一个更强的条件。

### 选择$\alpha$的充分条件——GoldStein Condition

**Armijo Condition**: $\phi(\alpha_k) \leq l_k(\alpha) = \phi(0) + c_1 \alpha \nabla f(x_k)^T d_k , c_1 \in (0, 1)$

**GoldStein Condition**:`

在满足 Armijo Condition 的基础上，我们还需要满足

![img](https://img2023.cnblogs.com/blog/3436855/202405/3436855-20240518011604436-1875832688.png)

$$
\phi(\alpha_k) \geq l_k(\alpha) = \phi(0) + c_2 \alpha \nabla f(x_k)^T d_k , c_2 \in (0, 1) \\
0 < c_2 < c_1 < 1
$$

初衷是为了保证$\alpha_k$不要太小，但是当区间比较窄的时候，容易失去效果。

### 选择$\alpha$的充分条件——Wolfe Condition

![img](https://img2023.cnblogs.com/blog/3436855/202405/3436855-20240518142842182-1362476063.png)

$$
\phi(\alpha_k) \leq l_k(\alpha) = \phi(0) + c_1 \alpha \nabla f(x_k)^T d_k , c_1 \in (0, 1) \\
\nabla f(x_k + \alpha_k d_k)^T d_k \geq c_2 \nabla f(x_k)^T d_k , c_2 \in (c_1, 1)
$$

用了一个相比于 GoldStein Condition 更加宽松的条件。

### 证明充分性

**Zoutendijk 定理**:

1. 如果$f$下有界，且连续可微
2. $\nabla f$ Lipschitz 连续，即任意$x, y$,存在$L$，使得$\| \nabla f(x) - \nabla f(y) \| \leq L \| x - y \|$，可以联系微分中值定理，用于限制梯度的范数。

   Lipschitz 连续是比连续和一致连续更强的条件。

3. $d_k$是一个下降方向，即$\nabla f(x_k)^T d_k < 0$
4. Wolfe Condition 满足于$\alpha_k$，则
   $$
    \phi(\alpha_k) \leq l_k(\alpha) = \phi(0) + c_1 \alpha \nabla f(x_k)^T d_k , c_1 \in (0, 1) \\
    \nabla f(x_k + \alpha_k d_k)^T d_k \geq c_2 \nabla f(x_k)^T d_k , c_2 \in (c_1, 1)
   $$

则有 $\sum_{k=0}^\infin \cos^2 \theta_k \| \nabla f_k \|^2 \leq \infin$

说明：由于$\sum_{k=0}^\infin \cos^2$ 一定是发散的，因此我们可以得到收敛性，即$\lim_{k \to \infin} \| \nabla f_k \| = 0$, 即梯度收敛到 0，即$f(x_k)$收敛到最优解。

证明略
