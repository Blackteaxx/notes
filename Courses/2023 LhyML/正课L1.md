- Concepts of Machine Learning
- Concepts of Deep Learning

## Concepts of ML

除了 Supervised Learning, Unsupervised Learning, ML 仍然有很多非常广泛的任务，structure Learning

1. model: $f(x;\Theta)$用来描述一组函数族

2. loss: $L(\Theta)$用来描述一组参数的好坏，损失函数的选取决定了最终模型的选取

   - MSE
   - KL divergence

3. optimization: $\Theta^* = \arg\min_{\Theta} L(\Theta)$
   - Gradient Descent: $\Theta_{t+1} = \Theta_t - \eta \nabla L(\Theta_t)$参见最优化笔记，在指定迭代次数或者收敛条件下，找到最优解，但是 GD 最大的问题不是 Local minima
   - SGD
   - Adam

### Example

一个时序预测的例子，给定一个时序数据，预测未来的数据

比较经典的从 Linear Model 开始，然后到 Nonlinear Model

> 由模型选择带来的问题，叫做**Model Bias**: $bias = (\mathbb{E}[f(x;D)] - f(x))^2$

可以使用分段线性模型（例如 RELU）叠加来近似任意函数，这就是**Universal Approximation Theorem**

$$
curve = \text{const.} + \sum_{i=1}^n c \cdot \frac{1}{1 + \exp(-w_i \cdot x - b_i)}
$$

通过调整参数$c, w, b$可以拟合任意函数

分段线性模型有

- Sigmoid Function
- ReLU
- Tanh
- ...

因此实际上，我们可以通过叠加分段线性模型来拟合任意函数，形式为

$$
y = b + \sum_{i=1}^n c_i \cdot \sigma(b_i + \sum_{j=1}^m w_{ij} \cdot x_j)
$$

这就是神经网络的基本结构

$$
r = x W + b= \begin{bmatrix}
   x_1 & x_2 & x_3
\end{bmatrix}
\begin{bmatrix}
   w_{11} & w_{12} & w_{13} \\
   w_{21} & w_{22} & w_{23} \\
   w_{31} & w_{32} & w_{33}
\end{bmatrix} +
\begin{bmatrix}
   b_1 & b_2 & b_3
\end{bmatrix}
$$

$$
h = \sigma(r)
$$

$$
y = h W_{hy} + b_{hy}
$$

然而对于所有$N$个数据计算损失函数的梯度是非常耗时的，因此我们可以使用**Mini-batch**的方法，每次只计算一部分数据的梯度

### Structure

因此，Deep Learning 最核心的概念就是

- Model defined by a set of functions
- Loss function
- Optimization
