## Image Classification

Image: 3-D Tensor `(channel, height, width)`

但是这样一个向量非常的长，使用 DNN 有很多参数，非常容易过拟合

因此我们需要对模型的架构实现适应图片特点的改进

- Observation 1：关键点识别/**局部特征模式识别**

- Simplification 1: Receptive Field, 每一个感受野的神经元只关注局部的特征（一个图片中的 patch），每一个局部的特征可以重叠，可以进入不同参数的神经元

  - Typical Setting: (kernel size) 3x3 但是是否会丢失信息？特征在 9 个像素就会表现出来吗？Stride = 1 / 2

- Observation 2：**平移不变性**，同样一个 pattern 可能出现在不同的位置

- Simplification 2: Parameter Sharing, 一个卷积核可以应用到整个图片上，减少参数数量

  - Typical Setting: 每一个卷积核的参数共享和计算设置的是 **Hadamard Product**，即加权求和

## Convolutional Layer

我们可以将卷积核的参数看作是一个 filter，用来提取图片的特征

每一个的 filter 的参数量为 $k \times k \times c$，其中 $k$ 是卷积核的大小，$c$ 是图片的通道数，一共有 $\text{num\_conv}$ 个卷积核

图片通过卷积核的卷积操作后，得到的是一个 feature map，feature map 的大小为 $(n - k + 1) \times (n - k + 1) \times \text{num\_conv}$，其中 $n$ 是图片的大小

- Observation 3：一个图片 Subsampling 后不会改变图片的 object

- Simplification 3: Pooling-Max / Average Pooling, 通过取一个区域的最大值或者平均值来减少图片的大小，减少参数数量

  - Typical Setting: 2x2 pooling with stride 2

## Application: Playing Go

Why CNN for Go?

- Some patterns are important but smaller than the whole board

- Same Patterns can appear anywhere on the board

## 辅：为什么使用 valid set 效果不好

- Training Set $\mathcal{D}_{train}$

  - 假设有三个模型 $M_1, M_2, M_3$

- Validation Set $\mathcal{D}_{valid}$
  - 用来选择模型，计算三个模型的 validation error、

这个过程也可以看作是一个训练的过程，此时的模型集合是 $\{M_1, M_2, M_3\}$

$$
P(\mathcal{D}_{valid}\ is \ bad) \leq |\mathcal{H}_{val}| \cdot 2\exp(-2N_{val}\epsilon^2)
$$

如果 Validation Set 模型数目过于多，模型过于复杂，那么也会导致效果不好

## Why Deep?

假设空间（模型复杂性）与训练集 LOSS 的关系

![img](https://img2023.cnblogs.com/blog/3436855/202408/3436855-20240813171023061-761174535.png)

- Review：Why Hidden Layer？：可以使用一个 Hidden Layer 来拟合一个非线性的函数

那为什么要使用 Deep？照这样说事实上只需要一个 Hidden Layer 就可以了

下列任务是一个类似作业 2 的语音音素识别任务，输入是一个音频，输出是一个音素

![img](https://img2023.cnblogs.com/blog/3436855/202408/3436855-20240813171652673-1363558076.png)

但是这样比较参数量没有保持不变

![img](https://img2023.cnblogs.com/blog/3436855/202408/3436855-20240813172203311-1675422656.png)

**实际上结论是：Deep 的架构可以以更少的参数量来拟合一个复杂的函数**

### Analogy： Logic Circuit

对于一个$d$个输入的逻辑电路，我们可以使用 $2^d$ 个门放在一层来拟合一个任意的函数

但是如果我们使用两层的话，我们可以使用 $2d$ 个门来拟合一个任意的函数

Deep networks outperform shallow networks in terms of the number of parameters needed to represent a function

## Spatial Transformer Layer

- CNN 的局限性：对于平移、旋转、缩放等变换不具有不变性

![img](https://img2023.cnblogs.com/blog/3436855/202408/3436855-20240813173807324-264084501.png)

- 如何做到这样的转换？

Gerneral Layer：$a_{nm}^l = \sum_{i=1}^{N_{l-1}} \sum_{j=1}^{M_{l-1}} w_{ij}^l a_{ij}^{l-1}$

而各种图片的变换，实际上是对其中的参数进行设置

![img](https://img2023.cnblogs.com/blog/3436855/202408/3436855-20240813181935559-1596007424.png)

- Expansion, Compression, Translation

  - Expansion/Compression：![img](https://img2023.cnblogs.com/blog/3436855/202408/3436855-20240813182047874-928161801.png)

- Rotation

  $$
  \begin{aligned}
  \begin{bmatrix}
  x' \\
  y'
  \end{bmatrix} &= \begin{bmatrix}
  \cos(\theta) & -\sin(\theta) \\
  \sin(\theta) & \cos(\theta)
  \end{bmatrix} \begin{bmatrix}
  x \\
  y
  \end{bmatrix} + \begin{bmatrix}
  t_x \\
  t_y
  \end{bmatrix}
  \end{aligned}
  $$

于是我们可以通过这样的变换来实现矩阵的旋转，**神经元需要学习的参数实际上就是线性变换的矩阵**

但是可能会存在非整数的情况，这样的情况并不可微，因此我们需要使用 **bilinear interpolation 来进行处理**

（这个过程很有意思！这个可视化是怎么做出来的？）
