## 回顾推荐系统链路

![img](https://img2023.cnblogs.com/blog/3436855/202406/3436855-20240605191751394-1535600093.png)

排序的依据是预估点击率、点赞率、收藏率等分数

最终依据分数的融合分数进行排序

## 多目标模型

![img](https://img2023.cnblogs.com/blog/3436855/202406/3436855-20240605192231105-1398580305.png)

对于每一个任务，损失函数就是一个交叉熵，最后加权求和

![img](https://img2023.cnblogs.com/blog/3436855/202406/3436855-20240605192411139-686127110.png)

- 困难：类别不平衡
  - 点击和非点击、收藏和非收藏的比例不一样
  - 浪费了大量的计算资源
- 解决方法
  - down-sampling

降采样之后需要进行预估值校准，因为由于 down-sampling，预估值会偏大

- 真实点击率: $p*{\text{true}} \approx \frac{n*+}{n*+ + n*-} $
- 预估点击率: $p_{\text{pred}} \approx \frac{n_+}{n_+ + \alpha n_-}$
- **校准公式**：$p_{\text{calibrated}} = \frac{p_{\text{pred}}}{p_{\text{pred}} + \frac{1-p_{\text{pred}}}{\alpha}}$

## Multi-gate Mixture-of-Experts(MMoE)

将三个不共享参数的神经网络称为三个专家网络，将三个专家网络的输出进行加权求和（加权参数由网络得出），得到最终的输出

![img](https://img2023.cnblogs.com/blog/3436855/202406/3436855-20240605193029791-76458462.png)

更上层的结构是一个神经网络，用于融合学习各个指标

![img](https://img2023.cnblogs.com/blog/3436855/202406/3436855-20240605193150715-1597103751.png)

MMoE 会有一个问题：极化现象

即 softmax 的输出会趋向于一个为 1，其他为 0，这就标志着没有使用 Mixture-of-Experts

解决的方案：dropout output of softmax layer
