## Background

统计学习的三要素包括模型、策略、算法。

统计学习方法概括如下：

假设数据是由独立同分布产生的

概念：是一个从样本空间$\mathcal{X}$到标记空间$\mathcal{Y}$的映射，如果对于任何的样例$(x,y)$都有$c(x)=y$，那么称作$c$为目标概念，目标概念的集合为目标概念类，记为$\mathcal{C}$。

1. 模型：假设要学习的模型属于某一种函数的集合，称为假设空间(Hypothesis Space)
2. 策略：应用某一个评价准则，从从假设空间选取一个最优的模型，使得已知数据和位置数据在模型下有最佳的预测
3. 算法：最优模型的选取有算法实现

### 模型

Mark:不正式的讲，我们在建模的时候，第一步就是要选择一个特定的模型比如 SVM。一旦选择了一个模型，就相当于我们选择了一个假设空间$\mathcal{H}$。在一个假设空间里，我们通常会有无数种不同的解，一个优化算法（比如梯度下降法）做的事情就是从中选择最好的一个解或者多个解，当然优化过程要依赖于样本数据。

正式的讲，假设空间$\mathcal{H}$是一个函数集合，从输入空间映射至输出空间的函数。

### 策略

#### 风险函数

我们可以定义损失函数，损失函数越低模型就越好，可以考察损失函数的期望值,被定义为**风险函数/期望损失**。

$$
R_{\text{exp}}(f) = \mathbb{E}_p[L(y, f(x))]= \int_{x,y} L(y, f(x)) p(x, y) \text{d} x \text{d}y
$$

然而， 我们一般不能获取到 joint 分布，因此我们经常使用模型$f(x)$关于训练集的**经验风险/经验损失**

$$
R_{\text{emp}}(f) = \frac{1}{N} \sum L(y_i, f(x_i))
$$

当$N \to \infin$时，根据大数定律，经验损失趋近于期望损失，然而实际情况下训练集样本量很少，需要对经验风险进行矫正，这样就涉及到两种基本策略：**经验风险最小化和结构风险最小化**。

**经验风险最小化**就是$\arg \min_{f \in \mathcal{F}} R_{\text{emp}}(f)$, 当样本容量很小的时候，会产生 over-fitting。

结构风险最小化就是$\arg \min_{f \in \mathcal{F}} \frac{1}{N} \sum L(y_i, f(x_i)) + \lambda J(f)$，其中$J(f)$时定义在$f$上的泛函，可以被称为正则项/罚项。**结构风险最小化等价于正则化。**

#### 泛化能力

我们在训练集上训练出来的模型，我们希望它在未知的数据上也能有很好的表现，这就是泛化能力。

然而在实践中，很难直接获取泛化能力，我们可以使用获取的测试集上的损失来近似泛化能力。

这样的评价指标一般是不可靠的，因为测试集是有限的，我们定义**泛化误差(gernerlization error)**，也就是期望风险为

$$
R_{\text{exp}}(f) = \mathbb{E}_p[L(y, f(x))]= \int_{x,y} L(y, f(x)) p(x, y) \text{d} x \text{d}y
$$

分析学习方法的泛化能力常常通过分析泛化误差上界进行。

## MLE, MAP, Bayesian

获得数据形式$X = (x_1, \dots, x_N)^T_{N \times P}$

- **MLE**

  假设$x_i$ ~ $p(x;\theta)$

  $$
    \theta_{MLE} = \arg \max_\theta p(X;\theta)
  $$

  当函数 convex 时，可以使用充要条件求梯度=0；当函数 non-convex 时，可以使用[EM 算法](https://www.cnblogs.com/Blackteaxx/p/18178802)迭代求解。

  在一定条件下，**MLE 和经验风险最小化等价**，详见《统计学习方法》习题 1.2

- **MAP**

  $$
  \theta_{MAP} = \arg \max p(\theta | X) = \arg \max \frac{p(X | \theta) p(\theta)}{\int p(X, \theta) \text{d}\theta} = \arg \max p(X | \theta) p(\theta)
  $$

  MLE 和 MAP 的最重要的区别是是否将$\theta$也作为一个随机变量考察分布。

  **但是也在一定方面有联系**

1. 比如添加正则的回归和带高斯先验的回归实际上是等价的。

添加 L2 正则的回归，我们的优化目标为

$$
W_{MLE} = \arg \min -\log p(X | W) + \lambda \| W \|_2^2
$$

如果是带有高斯先验的 MAP，我们的优化目标为

$$
W_{MLE} = \arg \max p(X | W)p(W) =  \\
\arg \min - \log p(X| W)p(W) = \\
 \arg \min - \log p(X| W) - \log p(W)
$$

那么最终第二项会形成一个$\| W |_2$的状态

- 样本量逐渐增大时，MAP 逐渐等价于 MLE

$$
\lim_{N \to \infin} - \log p(X| W) - \log p(W) = \lim_{N \to \infin} - \sum \log p(x_i | W) - \log p(W) =  \\ - \sum \log p(x_i | W)
$$

MLE 比 MAP 更容易过拟合。因为 MLE 在求解最优$\theta$时，没有对$\theta$有先验的指导，因此 X 中包括了一些**outlier**的数据样本时，就会很轻易让 MLE 去拟合 outlier 样本。而 MAP 加入了对 θ 的先验指导，例如 L2 正则化，那么就不易过拟合了。

- Bayesian Predict

  将$\theta$也看作**一个分布**，但是在预测的时候并不求解最优的一个参数值$\theta$，而是直接使用$\theta$的分布对**所有模型的结果做加权求和**。（假设新获取的数据$\hat{x}$与$X$独立同分布）

  $$
      p(\hat{x}|X) = \int p(\hat{x},\theta | X) \text{d} \theta = \int p(\hat{x} | \theta) p(\theta | X) \text{d} \theta
  $$

  _Mark: $p(\hat{x},\theta | X) = \frac{p(\hat{x},\theta, X)}{p(X)} = \frac{p(\hat{x} | \theta, X) p(\theta, X)}{p(X)} = \frac{p(\hat{x} | \theta) p(\theta, X)}{p(X)} = p(\hat{x} | \theta) p(\theta | X) $_
  为了求解$p(\theta | X)$，就需要对联合分布进行求解：

  $$
    p(\theta | X) = \frac{p(X | \theta)p(\theta)}{\int p(X | \theta) p(\theta) \text{d} \theta}
  $$

  对于联合分布，是非常难以求解的，在经典应用 Markov Random Field 中就有提及，我们可以使用[确切推断/近似推断](https://www.cnblogs.com/Blackteaxx/p/18180183)来求解这样一个联合概率，比如说动态规划 or MCMC or 变分推断。
