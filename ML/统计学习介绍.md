## Background

统计学习的三要素包括模型、策略、算法。

统计学习方法概括如下：

假设数据是由独立同分布产生的

概念：是一个从样本空间$\mathcal{X}$到标记空间$\mathcal{Y}$的映射，如果对于任何的样例$(x,y)$都有$c(x)=y$，那么称作$c$为目标概念，目标概念的集合为目标概念类，记为$\mathcal{C}$。

1. 模型：假设要学习的模型属于某**一种函数的集合，称为假设空间(Hypothesis Space)**
2. 策略：应用某一个评价准则，从从假设空间选取一个最优的模型，使得已知数据和位置数据在模型下有最佳的预测
3. 算法：最优模型的选取有算法实现

### 假设空间

Mark:不正式的讲，我们在建模的时候，第一步就是要选择一个特定的模型比如 SVM。一旦选择了一个模型，就相当于我们选择了一个假设空间$\mathcal{H}$。在一个假设空间里，我们通常会有无数种不同的解，一个优化算法（比如梯度下降法）做的事情就是从中选择最好的一个解或者多个解，当然优化过程要依赖于样本数据。

正式的讲，假设空间$\mathcal{H}$是一个函数集合，从输入空间映射至输出空间的函数。

而满足训练集的假设空间称为**版本空间**，版本空间是假设空间的一个子集，是与训练集一致的假设的集合。

### Inductive Bias

如果版本空间中有多个假设，那么我们需要选择一个假设，这就涉及到**归纳偏置**。归纳偏置是学习算法在学习过程中对**某种类型假设的偏好**。

然而，确定一个归纳偏置是有歧义的，因为无法定义$简单$、$平滑$这类概念。因此，归纳偏置是一个**经验性的偏好**。

我们观察 NFL 定理，我们标记为$H$为假设空间，$\mathcal{L_a}$为学习算法，$f$为目标函数，$P(h | X, \mathcal{L_a})$为学习算法在数据集下产生的假设$h$的概率

$$
E(\mathcal{L_a} | X, f) = \sum_h \mathbb{E} [\mathbb{I}(h(x) \neq f(x)) | h] P(h | X, \mathcal{L_a})
$$

### 策略

#### 风险函数

我们可以定义损失函数，损失函数越低模型就越好，可以考察损失函数的期望值,被定义为**风险函数/期望损失**。

$$
R_{\text{exp}}(f) = \mathbb{E}_p[L(y, f(x))]= \int_{x,y} L(y, f(x)) p(x, y) \text{d} x \text{d}y
$$

然而， 我们一般不能获取到 joint 分布，因此我们经常使用模型$f(x)$关于训练集的**经验风险/经验损失**

$$
R_{\text{emp}}(f) = \frac{1}{N} \sum L(y_i, f(x_i))
$$

当$N \to \infin$时，根据大数定律，经验损失趋近于期望损失，然而实际情况下训练集样本量很少，需要对经验风险进行矫正，这样就涉及到两种基本策略：**经验风险最小化和结构风险最小化**。

**经验风险最小化**就是$\arg \min_{f \in \mathcal{F}} R_{\text{emp}}(f)$, 当样本容量很小的时候，会产生 over-fitting。

结构风险最小化就是$\arg \min_{f \in \mathcal{F}} \frac{1}{N} \sum L(y_i, f(x_i)) + \lambda J(f)$，其中$J(f)$时定义在$f$上的泛函，可以被称为正则项/罚项。**结构风险最小化等价于正则化。**

#### 泛化能力

我们在训练集上训练出来的模型，我们希望它在未知的数据上也能有很好的表现，这就是泛化能力。

然而在实践中，很难直接获取泛化能力，我们可以使用获取的测试集上的损失来近似泛化能力。

这样的评价指标一般是不可靠的，因为测试集是有限的，我们定义**泛化误差(gernerlization error)**，也就是期望风险为

$$
R_{\text{exp}}(f) = \mathbb{E}_p[L(y, f(x))]= \int_{x,y} L(y, f(x)) p(x, y) \text{d} x \text{d}y
$$

分析学习方法的泛化能力常常通过分析泛化误差上界进行。

### bias-variance tradeoff

我们使用偏差和方差两个指标来评价模型的能力。

假设我们训练的时候使用采样的数据集$D$，样本真实标记为$f(x)$，采样出的标记为$f(x) + \epsilon$，$f(x;D)$为训练出的模型，$\epsilon$为噪声

那么对于回归问题在平均意义下，算法的期望预测为

$$
\hat{f}(x) = \mathbb{E}_D [f(x;D)]
$$

那么我们可以定义模型的偏差为

$$
\text{bias}^2(x) = (\mathbb{E}_D [f(x;D)] - f(x))^2
$$

方差为

$$
\text{var}(x) = \mathbb{E}_D[(f(x;D) - \mathbb{E}_D[f(x;D)])^2]
$$

那么**期望泛化误差**为

$$
\mathbb{E}_D[(f(x;D) - f(x) - \epsilon)^2] = \text{bias}^2(x) + \text{var}(x) + \epsilon^2
$$

可以简单地理解为，当模型复杂度/训练轮数上升到一定程度时，偏差降低，但是方差上升，因此我们需要在偏差和方差之间取一个平衡。

## MLE, MAP, Bayesian

获得数据形式$X = (x_1, \dots, x_N)^T_{N \times P}$

- **MLE**

  假设$x_i$ ~ $p(x;\theta)$

  $$
    \theta_{MLE} = \arg \max_\theta p(X;\theta)
  $$

  当函数 convex 时，可以使用充要条件求梯度=0；当函数 non-convex 时，可以使用[EM 算法](https://www.cnblogs.com/Blackteaxx/p/18178802)迭代求解。

  在一定条件下，**MLE 和经验风险最小化等价**，详见《统计学习方法》习题 1.2

  同时，**MLE 等价于 最小化数据分布与真实分布的 KL 散度**

  $$
  \theta^* = \arg \min_\theta - \sum_{i=1}^N \log p_\theta(x_i ; \theta) \approx - \mathbb{E}_{x - p_{data}} [\log p_\theta(x ; \theta)] \\
  = \arg \min_\theta - \mathbb{E}_{x - p_{data}} [\log \frac{p_\theta(x ; \theta)}{p_{data}(x)}] \\
  = \arg \min_\theta - \int p_{data}(x) \log \frac{p_\theta(x ; \theta)}{p_{data}(x)} dx = \arg \min_\theta KL(p_{data} || p_\theta)
  $$

- **MAP**

  $$
  \theta_{MAP} = \arg \max p(\theta | X) = \arg \max \frac{p(X | \theta) p(\theta)}{\int p(X, \theta) \text{d}\theta} = \arg \max p(X | \theta) p(\theta)
  $$

  MLE 和 MAP 的最重要的区别是是否将$\theta$也作为一个随机变量考察分布。

  **但是也在一定方面有联系**

1. 比如添加正则的回归和带高斯先验的回归实际上是等价的。

添加 L2 正则的回归，我们的优化目标为

$$
W_{MLE} = \arg \min -\log p(X | W) + \lambda \| W \|_2^2
$$

如果是带有高斯先验的 MAP，我们的优化目标为

$$
W_{MLE} = \arg \max p(X | W)p(W) =  \\
\arg \min - \log p(X| W)p(W) = \\
 \arg \min - \log p(X| W) - \log p(W)
$$

那么最终第二项会形成一个$\| W |_2$的状态

- 样本量逐渐增大时，MAP 逐渐等价于 MLE

$$
\lim_{N \to \infin} - \log p(X| W) - \log p(W) = \lim_{N \to \infin} - \sum \log p(x_i | W) - \log p(W) =  \\ - \sum \log p(x_i | W)
$$

MLE 比 MAP 更容易过拟合。因为 MLE 在求解最优$\theta$时，没有对$\theta$有先验的指导，因此 X 中包括了一些**outlier**的数据样本时，就会很轻易让 MLE 去拟合 outlier 样本。而 MAP 加入了对 θ 的先验指导，例如 L2 正则化，那么就不易过拟合了。

- Bayesian Predict

  将$\theta$也看作**一个分布**，但是在预测的时候并不求解最优的一个参数值$\theta$，而是直接使用$\theta$的分布对**所有模型的结果做加权求和**。（假设新获取的数据$\hat{x}$与$X$在给定$\theta$条件下独立同分布）

  $$
      p(\hat{x}|X) = \int p(\hat{x},\theta | X) \text{d} \theta = \int p(\hat{x} | \theta) p(\theta | X) \text{d} \theta
  $$

  _Mark: $p(\hat{x},\theta | X) = \frac{p(\hat{x},\theta, X)}{p(X)} = \frac{p(\hat{x} | \theta, X) p(\theta, X)}{p(X)} = \frac{p(\hat{x} | \theta) p(\theta, X)}{p(X)} = p(\hat{x} | \theta) p(\theta | X) $_
  为了求解$p(\theta | X)$，就需要对联合分布进行求解：

  $$
    p(\theta | X) = \frac{p(X | \theta)p(\theta)}{\int p(X | \theta) p(\theta) \text{d} \theta}
  $$

  对于联合分布，是非常难以求解的，在经典应用 Markov Random Field 中就有提及，我们可以使用[确切推断/近似推断](https://www.cnblogs.com/Blackteaxx/p/18180183)来求解这样一个联合概率，比如说变量消除 or MCMC or 变分推断。
