## 参考

西瓜书 & 南瓜书
统计学习方法

## Background

集成学习的基本原理是：通过构建并结合多个学习器，可以获得比单个学习器更好的泛化性能。集成学习的核心思想是通过结合多个学习器的判断，可以获得比单个学习器更好的泛化性能。

在 PAC 学习框架中，如果存在一个多项式的学习算法能够学习一个概念，并且准确率很高，那么就称这样一个概念是强可学习的；如果准确率比随机猜测好，那么就称这样一个概念是弱可学习的。Schapire 和 Freund 证明了弱可学习概念和强可学习概念是等价的，即**一个概念是弱可学习的充分必要条件是这个概念是强可学习的**。

弱学习器可以是同质化的可以被叫做基学习），也可以是非同质化的（可以被叫做个体学习器）。

那么如何根据弱学习器构建强学习器就是集成学习的任务。集成学习的方法主要有两类：

- Bagging：并行化的集成学习方法，通过对训练数据集进行采样，然后训练多个弱学习器，最后通过投票的方式进行预测。
- Boosting：串行化的序列化学习方法，通过对训练数据集进行加权，然后训练多个弱学习器，最后通过加权投票的方式进行预测。

要获得好的集成，个体学习器应当 **“好而不同”**，即个体学习器应当有较高的 **准确率**，同时应当有较大的 **多样性**。

---

一个简单的分析：考虑二分类问题$y \in \{+1, -1 \}$, 所有基分类器的误差为

$$
P(h_i(x) \neq y) = \epsilon
$$

假设集成通过 T 个学习器绝对多数投票法确定，即

$$
F(x) = sign(\sum_{i=1}^{T}  h_i(x))
$$

假设基分类器错误率相互独立，那么根据 Hoeffding 不等式，集成分类器的错误率为

$$
P(F(x) \neq y) = \sum_{k=0}^{\lfloor \frac{T}{2} \rfloor} C_T^k (1-\epsilon)^k \epsilon^{T-k} \leq \exp(-\frac{1}{2} T (1-2\epsilon)^2)
$$

上述第一个等式是由于当 $k > \lfloor \frac{T}{2} \rfloor$ 时，$k$ 个分类器错误，集成分类器无法正确分类。

由上述不等式可知

1. 随着个体分类器数目的增加，集成分类器的错误率会指数级下降
2. $\epsilon = 0.5$的分类器是无用的，但是$\epsilon \leq 0.5$的分类器是有用的，$\epsilon \geq 0.5$也是有用的。

---

## Boosting-AdaBoost

对集成模型采用加性模型(additive model)的形式，即

$$
H(x) = \sum_{t=1}^{T} \alpha_t h_t(x)
$$

### 算法流程

1. 初始化训练数据的权值分布，对于训练数据集$D$，每个样本的权值为$D_0(i) = \frac{1}{N}$，其中$N$为样本数目。
2. 对于 $t = 1, 2, \cdots, T$:

   1. 使用权值分布$D_t$的训练数据集学习得到基分类器$h_t(x)$
   2. 计算$h_t(x)$在训练数据集上的分类误差率
      $\epsilon_t = P(h_t(x) \neq y) = \sum_i D_t(i)\mathbb{I}(h_t(x_i) \neq y_i)$
   3. 计算$h_t(x)$的系数$\alpha_t = \frac{1}{2} \log \frac{1-\epsilon_t}{\epsilon_t}$
   4. 更新训练数据集的权值分布
      $$
      D_{t+1} = \frac{D_t(i) \exp(-\alpha_t y_i h_t(x_i))}{Z_t}
      $$
      其中$Z_t$是规范化因子，使得$D_{t+1}$成为一个概率分布。

3. 构建基分类器的线性组合
   $$
   f(x) = \sum_{t=1}^{T} \alpha_t h_t(x)
   $$
4. 得到最终分类器
   $$
   h(x) = sign(f(x)) = sign(\sum_{t=1}^{T} \alpha_t h_t(x))
   $$

### 算法分析

1. 系数$\alpha_t$表示了基分类器$h_t(x)$的重要性，当$\epsilon_t$越小，$\alpha_t$越大，即分类误差率越小的基分类器在最终分类器中的作用越大。
2. 权值分布$D_t$表示了不同样本在不同基分类器中的重要性，分类错误的样本在下一轮中的权值越大，即
   $$
   D_{t+1}(i) = \begin{cases}
   \frac{D_t(i) \exp(-\alpha_t)}{Z_t} & h_t(x_i) = y_i \\
   \frac{D_t(i) \exp(\alpha_t)}{Z_t} & h_t(x_i) \neq y_i
   \end{cases}
   $$

### 前向分布算法

考虑 additive model 的形式：

$$
H(x) = \sum_{t=1}^{T} \alpha_t h_t(x;\gamma_t)
$$

其中所有的$\alpha_t$与$\gamma_t$都需要学习，非常困难，因此考虑**前向分步算法**，即每次只学习一个基分类器$h_t(x)$，并且学习$\alpha_t$。

$$
H_t(x) = H_{t-1}(x) + \alpha_t h_t(x)
$$

只需要优化

$$
(\alpha_t, h_t) = \arg \min_{\alpha, h} \sum_{i=1}^{N} \mathcal{L}(y_i, H_{t-1}(x_i) + \alpha h(x_i))
$$

得到$\alpha_t$和$h_t(x)$后，更新$H_t(x) = H_{t-1}(x) + \alpha_t h_t(x)$。

### 前向分布算法与 AdaBoost

> **AdaBoost 是前向分布算法的一个特例，AdaBoost 的损失函数为指数损失函数**：

> Proof Start

#### Step 1: 化简损失函数

考虑指数损失函数

$$
\mathcal{L}(y, f(x)) = \exp(-yf(x))
$$

那么定义在 additive model 上的损失函数即为

$$
\mathcal{L}(y, H(x)) = \mathbb{E}_{D}[exp(-yH(x))] = \\
\mathbb{E}_{D}[exp(-y(H_{t-1}(x) + \alpha_t h_t(x)))] = \\
\sum_i^N D(i) \exp(-y_i(H_{t-1}(x_i) + \alpha_t h_t(x_i))) = \\
\sum_i^N D(i) \exp(-y_iH_{t-1}(x_i)) \exp(-\alpha_t y_i h_t(x_i))
$$

我们具体考虑$\exp(-\alpha_t y_i h_t(x_i))$，当$h_t(x_i) = y_i$时，$\exp(-\alpha_t y_i h_t(x_i)) = \exp(-\alpha_t)$，当$h_t(x_i) \neq y_i$时，$\exp(-\alpha_t y_i h_t(x_i)) = \exp(\alpha_t)$，因此

$$
\exp(-\alpha_t y_i h_t(x_i)) = \begin{cases}
\exp(-\alpha_t) & h_t(x_i) = y_i \\
\exp(\alpha_t) & h_t(x_i) \neq y_i
\end{cases}
$$

于是

$$
\exp(-\alpha_t y_i h_t(x_i)) = \exp(-\alpha_t) \mathbb{I}(h_t(x_i) = y_i) + \exp(\alpha_t) \mathbb{I}(h_t(x_i) \neq y_i) = \\
\exp(-\alpha_t) (1 - \mathbb{I}(h_t(x_i) \neq y_i)) + \exp(\alpha_t) \mathbb{I}(h_t(x_i) \neq y_i) = \\
\exp(-\alpha_t) + (\exp(\alpha_t) - \exp(-\alpha_t)) \mathbb{I}(h_t(x_i) \neq y_i)
$$

那么损失函数可以写成

$$
\sum_i^N D(i) \exp(-y_iH_{t-1}(x_i)) \exp(-\alpha_t y_i h_t(x_i)) = \\
\sum_i^N D(i) \exp(-y_iH_{t-1}(x_i)) \exp(-\alpha_t) + (\exp(\alpha_t) - \exp(-\alpha_t)) \mathbb{I}(h_t(x_i) \neq y_i)
$$

#### Step 2：定义新的权值分布

由于$D(i) \exp(-y_iH_{t-1}(x_i))$与$h_t(x), \alpha_t$无关，**因此我们 denote**

$$
\hat{D}_t(i) = D(i) \exp(-y_iH_{t-1}(x_i))
$$

那么损失函数可以写成

$$
\sum_i^N D_t(i) \exp(-y_iH_{t-1}(x_i)) \exp(-\alpha_t) + (\exp(\alpha_t) - \exp(-\alpha_t)) \mathbb{I}(h_t(x_i) \neq y_i) = \\
\sum_i^N \hat{D}_t(i) ( \exp(-\alpha_t) + (\exp(\alpha_t) - \exp(-\alpha_t)) \mathbb{I}(h_t(x_i) \neq y_i) )
$$

#### Step 3：求解最优的$h_t(x)$和$\alpha_t$

**我们分别对$h_t(x)$和$\alpha_t$进行优化**，对于$h_t(x)$，我们可以得到

$$
h_t(x) = \arg \min_{h} \sum_i^N \hat{D}_t(i) (\exp(\alpha_t) - \exp(-\alpha_t)) \mathbb{I}(h(x_i) \neq y_i)
$$

假设$\exp(\alpha_t) - \exp(-\alpha_t) \geq 0$, 上述优化问题等价于

$$
h_t(x) = \arg \min_{h} \sum_i^N \hat{D}_t(i) \mathbb{I}(h(x_i) \neq y_i) =  \\ \arg \min_{h} \mathbb{E}_{\hat{D}}[\mathbb{I}(h(x_i) \neq y_i)] = \\  \arg \min_{h} \epsilon_t
$$

第二个等号由于$D_t$与$\hat{D}_t$只差一个归一化因子，因此等价。

即$h_t(x)$是在$\hat{D}_t$上的最小分类误差率的基分类器,

对于$\alpha_t$，我们使用偏导数可以得到

$$
\frac{\partial \mathcal{L}}{\partial \alpha} = - \exp(-\alpha) \sum_i^N \hat{D}_t(i) + \\ (\exp(\alpha_t) + \exp(-\alpha_t)) \sum_i^N \hat{D}_t(i) \mathbb{I}(h(x_i) \neq y_i) = 0
$$

化简得

$$
\frac{ \exp(-\alpha)}{\exp(\alpha_t) + \exp(-\alpha_t)} = \\
\frac{\sum_i^N \hat{D}_t(i) \mathbb{I}(h(x_i) \neq y_i)}{\sum_i^N \hat{D}_t(i)} = \\
\sum_i^N D_t(i) \mathbb{I}(h(x_i) \neq y_i) = \\
\epsilon_t
$$

那么最终得到

$$
\alpha_t = \frac{1}{2} \log \frac{1-\epsilon_t}{\epsilon_t}
$$

> Proof End

_Summary_：通过指数损失函数与前向分布算法，我们可以得到 AdaBoost 的算法流程。

## Boosting-提升树与 GDBT

更一般的集成损失函数形式为

$$
\mathcal{L}(H) = \mathbb{E}_D [\mathcal{l}(y, H(x))]
$$

我们通过$H_{t-1}$的损失函数值来求$H_t$的损失函数值，使用 Talyor Expansion(这里不理解，对一个函数的泰勒展开是什么？)，即

$$
l(y, H_t(x)) = l(y, H_{t-1}(x)) + \frac{\partial l(y, H(x))}{\partial H(x)} \bigg|_{H(x) = H_{t-1}(x)}(H_t(x) - H_{t-1}(x))
$$

损失函数可以被化简为

$$
\mathbb{E}_D [\mathcal{l}(y, H(x))] = \\
\mathbb{E}_D [\mathcal{l}(y, H_{t-1}(x))] + \underbrace{ \mathbb{E}_D [\frac{\partial \mathcal{l}(y, H(x))}{\partial H(x)} \bigg|_{H(x) = H_{t-1}(x)} \alpha h(x)] }_{\text{Residual}}
$$

于是，我们可以得到优化问题

$$
(\alpha_t, h_t) = \arg \min_{\alpha, h} \mathbb{E}_D [\frac{\partial \mathcal{l}(y, H(x))}{\partial H(x)} \bigg|_{H(x) = H_{t-1}(x)} \alpha h(x)]
$$

## Boosting 总结

在前向分布求解的过程中，实际上可以一定程度的看作是一个残差逼近的过程，即每次学习一个基分类器$h_t(x)$，使得当前模型$H_{t-1}(x)$的**残差最小**。

Boosting 使用**对样本重新赋权的方式**，使得分类错误的样本在下一轮中的权值更大，从而使得基分类器更加关注分类错误的样本，对不能处理权重的 learner，可以通过**对样本重采样**的方式来实现。

从偏差-方差分解的角度来看，Boosting 通过**减小偏差**的方式来提高模型的泛化性能。

## Bagging

虽然集成学习中的学习器独立性很难保证，但是可以通过其他手段保持学习器的多样性，Bagging 就是一种通过**对训练数据集进行采样**的方式来保持学习器的多样性。

使用 Bootrap Sampling 的方式，即对训练数据集进行有放回的采样，得到$T$个新的训练数据集。随后并行训练$T$个基分类器，最后通过投票/平均的方式进行预测。

Bootstrap Sampling 保证了每个基分类器的训练数据集是不同的，从而保证了基分类器的多样性。同时，也能保证有$\frac{1}{e}$的样本没有被采样到，**这部分样本可以用作验证集对泛化误差进行“包外估计”**。

$$
\lim \limits_{N \to \infty} (1 - \frac{1}{N})^N = \frac{1}{e}
$$

定义$D_t$为$h_t$的训练集，$H^{\text{oob}}(x)$为对样本$x$的包外预测，仅考虑那些未使用$x$的基分类器$h_t$，那么

$$
H^{\text{oob}}(x) = \arg \max_y \sum_{t=1}^T \mathbb{I}(h_t(x) = y) \mathbb{I}(x \notin D_t)
$$

Bagging 的泛化误差的包外估计可以定义为

$$
\epsilon^{\text{oob}} = \mathbb{E}_D [\mathbb{I}(H^{\text{oob}}(x) \neq y)]
$$

Bagging 更加注重**减小方差**。

## Bagging-Random Forest

Random Forest 是 Bagging 的一个扩展，通过**对特征进行随机采样(列采样)**，使得每个基分类器的训练数据集不仅在样本（行采样）上不同，而且在特征上也不同。

## Stacking

Stacking 是一种更加复杂的集成学习方法，通过**将多个基分类器的输出作为新的特征**，然后训练一个元分类器。

Stacking 通过**减小偏差**的方式来提高模型的泛化性能。

其实也就是一个**二层的模型**，第一层是多个基分类器，第二层是一个元分类器。
