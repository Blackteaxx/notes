## 概述

聚类算法是一种**无监督学习算法**，它将数据集中的数据划分为若干个不同的簇，使得**同一个簇内的数据相似度较高，不同簇之间的数据相似度较低**。聚类算法的目标是使得簇内的数据相似度尽可能高，簇间的数据相似度尽可能低。

常用的算法有 GMM、K-means（可以看作 Hard GMM）、DBSCAN 等各种算法。

我们可以将聚类算法分为两类：compactness 聚类和 connectivity 聚类。

compactness 聚类是指在数据空间中，将数据点聚集在一起，形成紧密的簇。

connectivity 聚类是指在数据空间中，将数据点连接在一起，形成簇。

## Spectral Clustering

谱聚类是一种基于图论的聚类算法，它将数据集中的数据看作是图中的节点，数据点之间的相似度看作是图中节点之间的边，使用图的分割方法来进行聚类。节点之间的相似度可以使用高斯核函数来计算（undirected graph）

$$
\text{W}(i, j) =
\begin{cases}
    \exp(-\frac{\|x_i - x_j\|^2}{2\sigma^2}) & \text{if } (i,j) \in \text{E} \\
    0 & \text{otherwise}
\end{cases}
$$

那么聚类算法可以定义为如下含义：

$$
\text{cut}(V) = \text{cut}(A_1, A_2, \cdots, A_k) =
\frac{1}{2} \sum_{i=1}^k \text{W}(A_i, V - A_i)
$$

然而在实际使用中还要考虑到每一个簇的大小，因此我们可以定义如下的目标函数：

$$
\text{cut}(V) = \text{cut}(A_1, A_2, \cdots, A_k) =
\frac{1}{2} \sum_{i=1}^k \frac{\text{cut}(A_i, V - A_i)}{\Delta}
$$

其中 $\Delta$ 可以是簇的大小，也可以是簇的度。我们一般使用 $\Delta = \text{degree}(A_k)$，其中 $\text{degree}(A_k)$ 是簇 $A_k$ 的度，$\text{degree}(A_k) = \sum_{d_i \in A_k} \sum_{d_j \in A_k} w_{ij}$。

我们定义 indicator vector $y$，其中

$$
y_{ij} \in \{0, 1\} \text{ if } i \in A_j \\
\sum_j y_{ij} = 1
$$

$$
Y = \begin{bmatrix}
y_{11} & y_{12} & \cdots & y_{1k} \\
y_{21} & y_{22} & \cdots & y_{2k} \\
\vdots & \vdots & \ddots & \vdots \\
y_{n1} & y_{n2} & \cdots & y_{nk}
\end{bmatrix}
$$

优化目标函数：

$$
\text{minimize} \quad \text{cut}(V) = \text{cut}(A_1, A_2, \cdots, A_k) \iff
\text{minimize} \sum_{i=1}^k \frac{\text{cut}(A_i, V - A_i)}{\sum_{i \in A_k} d_i}
$$

可以写成

$$
\hat{Y} = \arg \min_{Y} \text{cut}(V)
$$

而

$$
\text{Ncut}(V) = \sum_{k=1}^K \frac{\text{cut}(A_k, V - A_k)}{\sum_{i \in A_k} d_i}
$$

可以写成对角矩阵的迹的形式

$$
\text{Ncut}(V) = \text{tr} \begin{bmatrix}
    \quad W(A_1, V - A_1), & \quad 0, & \quad \cdots, & \quad 0 \\
    \quad 0, & \quad W(A_2, V - A_2), & \quad \cdots, & \quad 0 \\
    \quad \vdots, & \quad \vdots, & \quad \ddots, & \quad \vdots \\
    \quad 0, & \quad 0, & \quad \cdots, & \quad W(A_k, V - A_k)
\end{bmatrix}
\begin{bmatrix}
    \sum_{i \in A_1} d_i & 0 & \cdots & 0 \\
    0 & \sum_{i \in A_2} d_i & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & \sum_{i \in A_k} d_i
\end{bmatrix}^{-1}
$$

那么形式就变为了已知$W, Y$,求解上述两个矩阵的优化问题。

$$
Y^T Y(k \times k) = \sum_{i=1}^k y_i y_i^T
$$

其中 $y_i$ 是一个 $n$ 维的向量，$y_i = [0, 0, \cdots, 1, \cdots, 0, 0]$，其中 $1$ 在第 $k$ 个位置, 那么$y_i y_i ^T = [1]_{k,k}$

那么$Y^T Y$就是一个对角矩阵，对角线上的元素是簇的大小，即$N_k = | A_k | = \sum_{i \in A_k} 1$
